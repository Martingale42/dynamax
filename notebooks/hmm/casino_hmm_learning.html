
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Casino HMM: Learning (parameter estimation)</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gaussian HMM" href="gaussian_hmm.html" />
    <link rel="prev" title="Casino HMM: Inference (state estimation)" href="casino_hmm_inference.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.gif" class="logo" alt="logo">
  
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent" role="heading">
 <span class="caption-text">
  HMMs
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="casino_hmm_inference.html">
   Casino HMM: Inference (state estimation)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Casino HMM: Learning (parameter estimation)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gaussian_hmm.html">
   Gaussian HMM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="autoregressive_hmm.html">
   Autoregressive (AR) HMM Demo
  </a>
 </li>
</ul>
<p class="caption collapsible-parent" role="heading">
 <span class="caption-text">
  Linear Gaussian SSMs
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_gaussian_ssm/kf_tracking.html">
   Tracking an object using the Kalman filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_gaussian_ssm/kf_linreg.html">
   Online linear regression using Kalman filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_gaussian_ssm/lgssm_parallel_inference.html">
   Parallel filtering and smoothing in an LG-SSM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_gaussian_ssm/lgssm_learning.html">
   MAP parameter estimation for an LG-SSM using EM and SGD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_gaussian_ssm/lgssm_hmc.html">
   Bayesian parameter estimation for an LG-SSM using HMC
  </a>
 </li>
</ul>
<p class="caption collapsible-parent" role="heading">
 <span class="caption-text">
  Nonlinear Gaussian SSMs
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nonlinear_gaussian_ssm/ekf_ukf_spiral.html">
   Tracking a spiraling object using the extended / unscented Kalman filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nonlinear_gaussian_ssm/ekf_ukf_pendulum.html">
   Tracking a 1d pendulum using Extended / Unscented Kalman filter/ smoother
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nonlinear_gaussian_ssm/ekf_mlp.html">
   Online learning for an MLP using extended Kalman filtering
  </a>
 </li>
</ul>
<p class="caption collapsible-parent" role="heading">
 <span class="caption-text">
  Generalized Gaussian SSMs
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../generalized_gaussian_ssm/cmgf_logistic_regression_demo.html">
   Online Logistic Regression using conditional moments Gaussian filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../generalized_gaussian_ssm/cmgf_mlp_classification_demo.html">
   Online learning of an MLP Classifier using conditional moments Gaussian filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../generalized_gaussian_ssm/cmgf_poisson_demo.html">
   Fitting an LDS with Poisson Likelihood using conditional moments Gaussian filter
  </a>
 </li>
</ul>
<p class="caption collapsible-parent" role="heading">
 <span class="caption-text">
  API Documentation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html">
   State Space Model (Base class)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html#hidden-markov-model">
   Hidden Markov Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html#linear-gaussian-ssm">
   Linear Gaussian SSM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html#nonlinear-gaussian-gssm">
   Nonlinear Gaussian GSSM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html#generalized-gaussian-gssm">
   Generalized Gaussian GSSM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html#utilities">
   Utilities
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/probml/dynamax"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/probml/dynamax/main?urlpath=tree/docs/notebooks/hmm/casino_hmm_learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/probml/dynamax/blob/main/docs/notebooks/hmm/casino_hmm_learning.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sample-data-from-true-model">
   Sample data from true model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-with-gradient-descent">
   Learning with Gradient Descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-is-a-special-case-of-stochastic-gradient-descent">
     Gradient descent is a special case of
     <em>
      stochastic
     </em>
     gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-gradient-descent-with-mini-batches">
     Stochastic Gradient Descent with Mini-Batches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-maximization">
     Expectation-Maximization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-the-learning-curves">
     Compare the learning curves
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="casino-hmm-learning-parameter-estimation">
<h1>Casino HMM: Learning (parameter estimation)<a class="headerlink" href="#casino-hmm-learning-parameter-estimation" title="Permalink to this heading">¶</a></h1>
<p>This notebook continues the “occasionally dishonest casino” example from the preceding notebook.
There, we assumed we knew the parameters of the model: the probability of switching between fair and loaded dice and the probabilities of the different outcomes (1,…,6) for each die.</p>
<p>Here, our goal is <strong>learn these parameters from data</strong>.  We will sample data from the model as before, but now we will estimate the parameters using eith stochastic gradient descent (SGD) or expectation-maximization (EM).</p>
<p>The figure below shows the <em>graphical model</em>, complete with the parameter nodes.</p>
<p align="center">
  <img src="https://github.com/probml/dynamax/blob/main/docs/figures/hmmDgmPlatesY.png?raw=true">
</p>
The filled in nodes are those which are observed (i.e. the emissions), and the unfilled nodes are ones that must be inferred (i.e. the latent states and parameters).  
<p>In Dynamax, the <a class="reference external" href="https://probml.github.io/dynamax/api.html#dynamax.hidden_markov_model.CategoricalHMM"><code class="docutils literal notranslate"><span class="pre">CategoricalHMM</span></code></a> assumes conjugate, Dirichlet prior distributions on the model parameters. Let <span class="math notranslate nohighlight">\(K\)</span> denote the number of discrete states (<span class="math notranslate nohighlight">\(K=2\)</span> in the casino example, either fair or loaded), and let <span class="math notranslate nohighlight">\(C\)</span> the number of categories the emissions can assume (<span class="math notranslate nohighlight">\(C=6\)</span> in the casino example, the number of faces of each die). The priors are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi &amp;\sim \mathrm{Dir}(\alpha 1_K) \\
A_k &amp;\sim \mathrm{Dir}(\beta 1_K) \quad \text{for } k=1,\ldots, K \\
B_k &amp;\sim \mathrm{Dir}(\gamma 1_C) \quad \text{for } k=1,\ldots, K
\end{align*}\]</div>
<p>Thus, the full prior distribution is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\theta) &amp;= \mathrm{Dir}(\pi \mid \alpha 1_K) \prod_{k=1}^K \mathrm{Dir}(A_k \mid \beta 1_K) \, \mathrm{Dir}(B_k \mid \gamma 1_C)
\end{align*}\]</div>
<p>The hyperparameters can be specified in the <a class="reference external" href="https://probml.github.io/dynamax/api.html#dynamax.hidden_markov_model.CategoricalHMM"><code class="docutils literal notranslate"><span class="pre">CategoricalHMM</span></code></a> constructor..</p>
<p>The <strong>learning objective</strong> is to find parameters tham maximize the marginal probability,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\theta^\star &amp;= \text{arg max}_{\theta} \; p(\theta \mid y_{1:T}) \\
&amp;= \text{arg max}_{\theta} \; p(\theta, y_{1:T})
\end{align*}\]</div>
<p>This is called the <em>maximum a posteriori</em> (MAP) estimate. Dynamax supports two algorithms for MAP estimation: expectation-maximization (EM) and stochastic gradient descent (SGD), which are described below.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">¶</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">dynamax</span>
<span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;installing dynamax&#39;</span><span class="p">)</span>
    <span class="o">%</span><span class="k">pip</span> install -q dynamax[notebooks]
    <span class="kn">import</span> <span class="nn">dynamax</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jr</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">optax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vmap</span>

<span class="kn">from</span> <span class="nn">dynamax.hidden_markov_model</span> <span class="kn">import</span> <span class="n">CategoricalHMM</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
</pre></div>
</div>
</div>
</div>
</section>
<section id="sample-data-from-true-model">
<h2>Sample data from true model<a class="headerlink" href="#sample-data-from-true-model" title="Permalink to this heading">¶</a></h2>
<p>First we construct an HMM and sample data from it, just as in the preceding notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_states</span> <span class="o">=</span> <span class="mi">2</span>      <span class="c1"># two types of dice (fair and loaded)</span>
<span class="n">num_emissions</span> <span class="o">=</span> <span class="mi">1</span>   <span class="c1"># only one die is rolled at a time</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">6</span>     <span class="c1"># each die has six faces</span>

<span class="n">initial_probs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> 
                               <span class="p">[</span><span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">]])</span>
<span class="n">emission_probs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">],</span>    <span class="c1"># fair die</span>
                            <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="o">/</span><span class="mi">10</span><span class="p">]])</span>  <span class="c1"># loaded die</span>


<span class="c1"># Construct the HMM</span>
<span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># Initialize the parameters struct with known values</span>
<span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">initial_probs</span><span class="o">=</span><span class="n">initial_probs</span><span class="p">,</span>
                           <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">,</span>
                           <span class="n">emission_probs</span><span class="o">=</span><span class="n">emission_probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_batches</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="n">batch_states</span><span class="p">,</span> <span class="n">batch_emissions</span> <span class="o">=</span> \
    <span class="n">vmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">hmm</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num_timesteps</span><span class="p">))(</span>
        <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">num_batches</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;batch_states.shape:    </span><span class="si">{</span><span class="n">batch_states</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;batch_emissions.shape: </span><span class="si">{</span><span class="n">batch_emissions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>batch_states.shape:    (5, 5000)
batch_emissions.shape: (5, 5000, 1)
</pre></div>
</div>
</div>
</div>
<p>We’ll write a simple function to print the parameters in a more digestible format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_params</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">formatter</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;float&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{0:0.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">)})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial probs:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">initial</span><span class="o">.</span><span class="n">probs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transition matrix:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">transitions</span><span class="o">.</span><span class="n">transition_matrix</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;emission probs:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">emissions</span><span class="o">.</span><span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span> <span class="c1"># since num_emissions = 1</span>
    
<span class="n">print_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial probs:
[0.500 0.500]
transition matrix:
[[0.950 0.050]
 [0.100 0.900]]
emission probs:
[[0.167 0.167 0.167 0.167 0.167 0.167]
 [0.100 0.100 0.100 0.100 0.100 0.500]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-with-gradient-descent">
<h2>Learning with Gradient Descent<a class="headerlink" href="#learning-with-gradient-descent" title="Permalink to this heading">¶</a></h2>
<p>Perhaps the simplest learning algorithm is to directly maximize the marginal probability with gradient ascent. Since optimization algorithms are typically formulated as <em>minimization</em> algorithms, we will instead use gradient <em>descent</em> to solve the equivalent problem of <em>minimizing the negative log marginal probability</em>, <span class="math notranslate nohighlight">\(-\log p(y_{1:T}, \theta)\)</span>. On each iteration, we compute the objective, take its gradient, and update our parameters by taking a step in the direction of steepest descent.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Even though JAX code looks just like regular numpy code, it supports automatic differentiation, making gradient descent straightforward to implement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The first step is to randomly initialize new parameters. You can do that by calling <code class="docutils literal notranslate"><span class="pre">hmm.initialize(key)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">key</span></code> is a JAX pseudorandom number generator (PRNG) key. When no other keyword arguments are supplied, this function will return parameters randomly sampled from the prior.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since we expect the states to persist for some time, we add a little stickiness to the prior distribution on transition probabilities via the <code class="docutils literal notranslate"><span class="pre">transition_matrix_stickiness</span></code> hyperparameter. This hyperparameter changes the prior to,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_k &amp;\sim \mathrm{Dir}(\beta 1_K + \kappa e_k) \quad \text{for } k=1,\ldots, K
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa \in \mathbb{R}_+\)</span> is the stickiness paramter and <span class="math notranslate nohighlight">\(e_k\)</span> is the one-hot vector with a one in the <span class="math notranslate nohighlight">\(k\)</span>-th position.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span>
                     <span class="n">transition_matrix_stickiness</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fbgd_params</span><span class="p">,</span> <span class="n">fbgd_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Randomly initialized parameters&quot;</span><span class="p">)</span>
<span class="n">print_params</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Randomly initialized parameters
initial probs:
[0.793 0.207]
transition matrix:
[[0.978 0.022]
 [0.252 0.748]]
emission probs:
[[0.145 0.115 0.024 0.136 0.187 0.393]
 [0.010 0.119 0.416 0.325 0.068 0.061]]
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice that <code class="docutils literal notranslate"><span class="pre">initialize</span></code> returns two things, the parameters and their properties. Among other things, the properties allow you to specify which parameters should be learned. You can set the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> flag to False if you want to fix certain parmeters.</p>
</div>
<section id="gradient-descent-is-a-special-case-of-stochastic-gradient-descent">
<h3>Gradient descent is a special case of <em>stochastic</em> gradient descent<a class="headerlink" href="#gradient-descent-is-a-special-case-of-stochastic-gradient-descent" title="Permalink to this heading">¶</a></h3>
<p>Gradient descent is a special case of stochastic gradient descent (SGD) in which each iteration uses all the data to compute the descent direction for parameter updates. In contrast, SGD uses only a <em>minibatch</em> of data in each update. You can think of gradient descent as the special case where the minibatch is really the entire dataset. That’s why we sometimes call it <em>full batch</em> gradient descent. When you’re working with very large datasets (e.g. datasets with many sequences), however, minibatches can be very informative, and SGD can converge more quickly than full batch gradient descent.</p>
<p>Dynamax models have a <code class="docutils literal notranslate"><span class="pre">fit_sgd</span></code> function that runs SGD. If you want to run full batch gradient descent, all you have to do set <code class="docutils literal notranslate"><span class="pre">batch_size=num_batches</span></code>, as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fbgd_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">fbgd_params</span><span class="p">,</span> <span class="n">fbgd_losses</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">fit_sgd</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">,</span> 
                                       <span class="n">fbgd_props</span><span class="p">,</span> 
                                       <span class="n">batch_emissions</span><span class="p">,</span> 
                                       <span class="n">optimizer</span><span class="o">=</span><span class="n">optax</span><span class="o">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.95</span><span class="p">),</span>
                                       <span class="n">batch_size</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span> 
                                       <span class="n">num_epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> 
                                       <span class="n">key</span><span class="o">=</span><span class="n">fbgd_key</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent-with-mini-batches">
<h3>Stochastic Gradient Descent with Mini-Batches<a class="headerlink" href="#stochastic-gradient-descent-with-mini-batches" title="Permalink to this heading">¶</a></h3>
<p>Now let’s run it with stochastic gradient descent using a batch size of two sequences per mini-batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sgd_params</span><span class="p">,</span> <span class="n">sgd_param_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">sgd_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">sgd_params</span><span class="p">,</span> <span class="n">sgd_losses</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">fit_sgd</span><span class="p">(</span><span class="n">sgd_params</span><span class="p">,</span> 
                                     <span class="n">sgd_param_props</span><span class="p">,</span> 
                                     <span class="n">batch_emissions</span><span class="p">,</span> 
                                     <span class="n">optimizer</span><span class="o">=</span><span class="n">optax</span><span class="o">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.95</span><span class="p">),</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                     <span class="n">num_epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> 
                                     <span class="n">key</span><span class="o">=</span><span class="n">sgd_key</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fbgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;full batch GD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD (mini-batch size = 2)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Full Batch Gradient Descent Learning Curve&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8032d5f17642fc2a22665b547b33157c45283a15261edc9f9d30591daf9c7513.png" src="../../_images/8032d5f17642fc2a22665b547b33157c45283a15261edc9f9d30591daf9c7513.png" />
</div>
</div>
<p>As you can see, stochastic gradient descent converges much more quickly that full-batch gradient descent in this example. Intuitively, that’s because SGD takes multiple steps per <em>epoch</em> (i.e. each complete sweep through the dataset), whereas full-batch gradient descent takes only one.</p>
<p>The algorithms appear to have converged, but have they learned the correct parameters? Let’s see…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the paramters after learning</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Full batch gradient descent params:&quot;</span><span class="p">)</span>
<span class="n">print_params</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stochastic gradient descent params:&quot;</span><span class="p">)</span>
<span class="n">print_params</span><span class="p">(</span><span class="n">sgd_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Full batch gradient descent params:
initial probs:
[0.793 0.207]
transition matrix:
[[0.961 0.039]
 [0.292 0.708]]
emission probs:
[[0.162 0.142 0.115 0.131 0.145 0.306]
 [0.013 0.170 0.406 0.249 0.093 0.070]]

Stochastic gradient descent params:
initial probs:
[0.793 0.207]
transition matrix:
[[0.964 0.036]
 [0.344 0.656]]
emission probs:
[[0.165 0.143 0.124 0.130 0.150 0.287]
 [0.014 0.229 0.315 0.258 0.110 0.074]]
</pre></div>
</div>
</div>
</div>
<p>Ok, but not perfect!</p>
</section>
<section id="expectation-maximization">
<h3>Expectation-Maximization<a class="headerlink" href="#expectation-maximization" title="Permalink to this heading">¶</a></h3>
<p>The more traditional way to estimate the parameters of an HMM is by expectation-maximization (EM). EM alternates between two steps:</p>
<ul class="simple">
<li><p><strong>E-step:</strong> Inferring the posterior distribution of latent states <span class="math notranslate nohighlight">\(z_{1:T}\)</span> given the parameters <span class="math notranslate nohighlight">\(\theta = (\pi, A, B)\)</span>. This step essentially runs the HMM forward-backward algorithm from the preceding notebook!</p></li>
<li><p><strong>M-step:</strong> Updating the parameters to maximize the expected log probability.
By iteratively performing these two steps, the algorithm converges to a local maximum of the marginal probability, <span class="math notranslate nohighlight">\(p(y_{1:T}, \theta)\)</span>, and hence to a MAP estimate of the parameters.</p></li>
</ul>
<p>In practice, EM often converges much quicker than SGD, especially when the models are “nice” (e.g. constructed with exponential family emission distributions). That is why dynamax has closed form M-steps for a variety of HMMs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">em_params</span><span class="p">,</span> <span class="n">em_param_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">em_params</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">fit_em</span><span class="p">(</span><span class="n">em_params</span><span class="p">,</span> 
                                  <span class="n">em_param_props</span><span class="p">,</span> 
                                  <span class="n">batch_emissions</span><span class="p">,</span> 
                                  <span class="n">num_iters</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='400' class='' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [400/400 00:02&lt;00:00]
    </div>
    </div></div>
</div>
</section>
<section id="compare-the-learning-curves">
<h3>Compare the learning curves<a class="headerlink" href="#compare-the-learning-curves" title="Permalink to this heading">¶</a></h3>
<p>Finally, let’s compare the learning curve of EM to those of SGD. For comparison, we plot the loss associated with the true parameters that generated the data.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>To compare the log probabilities returned by <code class="docutils literal notranslate"><span class="pre">fit_em</span></code> to the losses returned by <code class="docutils literal notranslate"><span class="pre">fit_sgd</span></code>, you need to negate the log probabilities and divide by the total number of emissions. This is because optimization library defaults typically assume the loss is scaled to be <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the &quot;losses&quot; from EM </span>
<span class="n">em_losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_probs</span> <span class="o">/</span> <span class="n">batch_emissions</span><span class="o">.</span><span class="n">size</span> 

<span class="c1"># Compute the loss if you used the parameters that generated the data</span>
<span class="n">true_loss</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">hmm</span><span class="o">.</span><span class="n">marginal_log_prob</span><span class="p">,</span> <span class="n">params</span><span class="p">))(</span><span class="n">batch_emissions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">true_loss</span> <span class="o">+=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">true_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">true_loss</span> <span class="o">/</span> <span class="n">batch_emissions</span><span class="o">.</span><span class="n">size</span>

<span class="c1"># Plot the learning curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fbgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;full batch GD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD (mini-batch size = 2)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">em_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;EM&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_loss</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Params&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning Curve Comparison&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/16a7e3fe9226a0c0610b20188fe6833922c9c11fc6ac7eb6ac342729c3b99472.png" src="../../_images/16a7e3fe9226a0c0610b20188fe6833922c9c11fc6ac7eb6ac342729c3b99472.png" />
</div>
</div>
<p>Not only does EM converge much faster on this example (here, in only a handful of iterations), it also converges to a better estimate of the parameters. Indeed, it essentially matches the loss obtained by the parameters that truly generated the data. We see that its parameter estimates are nearly the same as the true parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_params</span><span class="p">(</span><span class="n">em_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial probs:
[0.628 0.372]
transition matrix:
[[0.909 0.091]
 [0.053 0.947]]
emission probs:
[[0.110 0.106 0.101 0.110 0.105 0.468]
 [0.171 0.173 0.171 0.164 0.164 0.157]]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>This notebook showed how to learn the parameters of an HMM using SGD (with full batch or with mini-batches) and EM. For many HMMs, especially the exponential family HMMs with exact M-steps implemented in Dynamax, EM tends to converge very quickly.</p>
<p>This notebook glossed over some important details:</p>
<ul class="simple">
<li><p>Both SGD and EM are prone to getting stuck in local optima. For example, if you change the key for the random initialization, you may find that the learned parameters are not as good. There are a few ways around that problem. One is to use a heuristic to initialize the parameters more intelligently. Another is to use many random initializations of the model and keep the one that achieves the best loss.</p></li>
<li><p>This notebook did not address the important question of <em>how to determine the number of discrete states</em>. We often use cross-validation for that purpose, as described next.</p></li>
</ul>
<p>So far, we have focused on HMMs with discrete emissions from a categorical distribution. The next notebook will illustrate a Gaussian HMM for continuous data. We will also discuss some of the concerns above.</p>
</section>
</section>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="casino_hmm_inference.html" title="previous page">Casino HMM: Inference (state estimation)</a>
    <a class='right-next' id="next-link" href="gaussian_hmm.html" title="next page">Gaussian HMM</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, and Kevin Murphy<br/>
        
            &copy; Copyright 2022, Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, and Kevin Murphy.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>