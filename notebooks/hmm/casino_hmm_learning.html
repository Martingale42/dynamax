
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Casino HMM: Learning (parameter estimation)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/hmm/casino_hmm_learning';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gaussian HMM: Cross-validation and Model Selection" href="gaussian_hmm.html" />
    <link rel="prev" title="Casino HMM: Inference (state estimation)" href="casino_hmm_inference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.gif" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.gif" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">HMMs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="casino_hmm_inference.html">Casino HMM: Inference (state estimation)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Casino HMM: Learning (parameter estimation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="gaussian_hmm.html">Gaussian HMM: Cross-validation and Model Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoregressive_hmm.html">Autoregressive (AR) HMM Demo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Gaussian SSMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../linear_gaussian_ssm/kf_tracking.html">Tracking an object using the Kalman filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linear_gaussian_ssm/kf_linreg.html">Online linear regression using Kalman filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linear_gaussian_ssm/lgssm_parallel_inference.html">Parallel filtering and smoothing in an LG-SSM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linear_gaussian_ssm/lgssm_learning.html">MAP parameter estimation for an LG-SSM using EM and SGD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linear_gaussian_ssm/lgssm_hmc.html">Bayesian parameter estimation for an LG-SSM using HMC</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Nonlinear Gaussian SSMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../nonlinear_gaussian_ssm/ekf_ukf_spiral.html">Tracking a spiraling object using the extended / unscented Kalman filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nonlinear_gaussian_ssm/ekf_ukf_pendulum.html">Tracking a 1d pendulum using Extended / Unscented Kalman filter/ smoother</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nonlinear_gaussian_ssm/ekf_mlp.html">Online learning for an MLP using extended Kalman filtering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generalized Gaussian SSMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../generalized_gaussian_ssm/cmgf_logistic_regression_demo.html">Online Logistic Regression using conditional moments Gaussian filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generalized_gaussian_ssm/cmgf_mlp_classification_demo.html">Online learning of an MLP Classifier using conditional moments Gaussian filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generalized_gaussian_ssm/cmgf_poisson_demo.html">Fitting an LDS with Poisson Likelihood using conditional moments Gaussian filter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../types.html">Terminology for types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">State Space Model (Base class)</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/probml/dynamax/main?urlpath=tree/docs/notebooks/hmm/casino_hmm_learning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/probml/dynamax/blob/main/docs/notebooks/hmm/casino_hmm_learning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/probml/dynamax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Casino HMM: Learning (parameter estimation)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-data-from-true-model">Sample data from true model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-with-gradient-descent">Learning with Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-is-a-special-case-of-stochastic-gradient-descent">Gradient descent is a special case of <em>stochastic</em> gradient descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-with-mini-batches">Stochastic Gradient Descent with Mini-Batches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization">Expectation-Maximization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-the-learning-curves">Compare the learning curves</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="casino-hmm-learning-parameter-estimation">
<h1>Casino HMM: Learning (parameter estimation)<a class="headerlink" href="#casino-hmm-learning-parameter-estimation" title="Link to this heading">#</a></h1>
<p>This notebook continues the “occasionally dishonest casino” example from the preceding notebook.
There, we assumed we knew the parameters of the model: the probability of switching between fair and loaded dice and the probabilities of the different outcomes (1,…,6) for each die.</p>
<p>Here, our goal is <strong>learn these parameters from data</strong>.  We will sample data from the model as before, but now we will estimate the parameters using either stochastic gradient descent (SGD) or expectation-maximization (EM).</p>
<p>The figure below shows the <em>graphical model</em>, complete with the parameter nodes.</p>
<p align="center">
  <img src="https://github.com/probml/dynamax/blob/main/docs/figures/hmmDgmPlatesY.png?raw=true">
</p>
The filled in nodes are those which are observed (i.e. the emissions), and the unfilled nodes are ones that must be inferred (i.e. the latent states and parameters).  
<p>In Dynamax, the <a class="reference external" href="https://probml.github.io/dynamax/api.html#dynamax.hidden_markov_model.CategoricalHMM"><code class="docutils literal notranslate"><span class="pre">CategoricalHMM</span></code></a> assumes conjugate, Dirichlet prior distributions on the model parameters. Let <span class="math notranslate nohighlight">\(K\)</span> denote the number of discrete states (<span class="math notranslate nohighlight">\(K=2\)</span> in the casino example, either fair or loaded), and let <span class="math notranslate nohighlight">\(C\)</span> the number of categories the emissions can assume (<span class="math notranslate nohighlight">\(C=6\)</span> in the casino example, the number of faces of each die). The priors are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi &amp;\sim \mathrm{Dir}(\alpha 1_K) \\
A_k &amp;\sim \mathrm{Dir}(\beta 1_K) \quad \text{for } k=1,\ldots, K \\
B_k &amp;\sim \mathrm{Dir}(\gamma 1_C) \quad \text{for } k=1,\ldots, K
\end{align*}\]</div>
<p>Thus, the full prior distribution is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\theta) &amp;= \mathrm{Dir}(\pi \mid \alpha 1_K) \prod_{k=1}^K \mathrm{Dir}(A_k \mid \beta 1_K) \, \mathrm{Dir}(B_k \mid \gamma 1_C)
\end{align*}\]</div>
<p>The hyperparameters can be specified in the <a class="reference external" href="https://probml.github.io/dynamax/api.html#dynamax.hidden_markov_model.CategoricalHMM"><code class="docutils literal notranslate"><span class="pre">CategoricalHMM</span></code></a> constructor..</p>
<p>The <strong>learning objective</strong> is to find parameters that maximize the marginal probability,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\theta^\star &amp;= \text{arg max}_{\theta} \; p(\theta \mid y_{1:T}) \\
&amp;= \text{arg max}_{\theta} \; p(\theta, y_{1:T})
\end{align*}\]</div>
<p>This is called the <em>maximum a posteriori</em> (MAP) estimate. Dynamax supports two algorithms for MAP estimation: expectation-maximization (EM) and stochastic gradient descent (SGD), which are described below.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%capture</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">dynamax</span>
<span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;installing dynamax&#39;</span><span class="p">)</span>
    <span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">dynamax</span><span class="p">[</span><span class="n">notebooks</span><span class="p">]</span>
    <span class="kn">import</span> <span class="nn">dynamax</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jr</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">optax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vmap</span>

<span class="kn">from</span> <span class="nn">dynamax.hidden_markov_model</span> <span class="kn">import</span> <span class="n">CategoricalHMM</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sample-data-from-true-model">
<h2>Sample data from true model<a class="headerlink" href="#sample-data-from-true-model" title="Link to this heading">#</a></h2>
<p>First we construct an HMM and sample data from it, just as in the preceding notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_states</span> <span class="o">=</span> <span class="mi">2</span>      <span class="c1"># two types of dice (fair and loaded)</span>
<span class="n">num_emissions</span> <span class="o">=</span> <span class="mi">1</span>   <span class="c1"># only one die is rolled at a time</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">6</span>     <span class="c1"># each die has six faces</span>

<span class="n">initial_probs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> 
                               <span class="p">[</span><span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">]])</span>
<span class="n">emission_probs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span>  <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">],</span>    <span class="c1"># fair die</span>
                            <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="o">/</span><span class="mi">10</span><span class="p">]])</span>  <span class="c1"># loaded die</span>


<span class="c1"># Construct the HMM</span>
<span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># Initialize the parameters struct with known values</span>
<span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">initial_probs</span><span class="o">=</span><span class="n">initial_probs</span><span class="p">,</span>
                           <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">,</span>
                           <span class="n">emission_probs</span><span class="o">=</span><span class="n">emission_probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_batches</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="n">batch_states</span><span class="p">,</span> <span class="n">batch_emissions</span> <span class="o">=</span> \
    <span class="n">vmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">hmm</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num_timesteps</span><span class="p">))(</span>
        <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">num_batches</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;batch_states.shape:    </span><span class="si">{</span><span class="n">batch_states</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;batch_emissions.shape: </span><span class="si">{</span><span class="n">batch_emissions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>batch_states.shape:    (5, 5000)
batch_emissions.shape: (5, 5000, 1)
</pre></div>
</div>
</div>
</div>
<p>We’ll write a simple function to print the parameters in a more digestible format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_params</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">formatter</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;float&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{0:0.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">)})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial probs:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">initial</span><span class="o">.</span><span class="n">probs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transition matrix:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">transitions</span><span class="o">.</span><span class="n">transition_matrix</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;emission probs:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">emissions</span><span class="o">.</span><span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span> <span class="c1"># since num_emissions = 1</span>
    
<span class="n">print_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial probs:
[0.500 0.500]
transition matrix:
[[0.950 0.050]
 [0.100 0.900]]
emission probs:
[[0.167 0.167 0.167 0.167 0.167 0.167]
 [0.100 0.100 0.100 0.100 0.100 0.500]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-with-gradient-descent">
<h2>Learning with Gradient Descent<a class="headerlink" href="#learning-with-gradient-descent" title="Link to this heading">#</a></h2>
<p>Perhaps the simplest learning algorithm is to directly maximize the marginal probability with gradient ascent. Since optimization algorithms are typically formulated as <em>minimization</em> algorithms, we will instead use gradient <em>descent</em> to solve the equivalent problem of <em>minimizing the negative log marginal probability</em>, <span class="math notranslate nohighlight">\(-\log p(y_{1:T}, \theta)\)</span>. On each iteration, we compute the objective, take its gradient, and update our parameters by taking a step in the direction of steepest descent.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Even though JAX code looks just like regular numpy code, it supports automatic differentiation, making gradient descent straightforward to implement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The first step is to randomly initialize new parameters. You can do that by calling <code class="docutils literal notranslate"><span class="pre">hmm.initialize(key)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">key</span></code> is a JAX pseudorandom number generator (PRNG) key. When no other keyword arguments are supplied, this function will return parameters randomly sampled from the prior.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since we expect the states to persist for some time, we add a little stickiness to the prior distribution on transition probabilities via the <code class="docutils literal notranslate"><span class="pre">transition_matrix_stickiness</span></code> hyperparameter. This hyperparameter changes the prior to,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_k &amp;\sim \mathrm{Dir}(\beta 1_K + \kappa e_k) \quad \text{for } k=1,\ldots, K
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa \in \mathbb{R}_+\)</span> is the stickiness paramter and <span class="math notranslate nohighlight">\(e_k\)</span> is the one-hot vector with a one in the <span class="math notranslate nohighlight">\(k\)</span>-th position.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span>
                     <span class="n">transition_matrix_stickiness</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fbgd_params</span><span class="p">,</span> <span class="n">fbgd_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Randomly initialized parameters&quot;</span><span class="p">)</span>
<span class="n">print_params</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">OverflowError</span><span class="g g-Whitespace">                             </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">hmm</span> <span class="o">=</span> <span class="n">CategoricalHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_emissions</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span>                      <span class="n">transition_matrix_stickiness</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="n">fbgd_params</span><span class="p">,</span> <span class="n">fbgd_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Randomly initialized parameters&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">print_params</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">)</span>

<span class="nn">File ~/work/dynamax/dynamax/dynamax/hidden_markov_model/models/categorical_hmm.py:174,</span> in <span class="ni">CategoricalHMM.initialize</span><span class="nt">(self, key, method, initial_probs, transition_matrix, emission_probs)</span>
<span class="g g-Whitespace">    </span><span class="mi">172</span> <span class="n">key1</span><span class="p">,</span> <span class="n">key2</span><span class="p">,</span> <span class="n">key3</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span> <span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">173</span> <span class="n">params</span><span class="p">,</span> <span class="n">props</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(),</span> <span class="nb">dict</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">174</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;initial&quot;</span><span class="p">],</span> <span class="n">props</span><span class="p">[</span><span class="s2">&quot;initial&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_component</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">initial_probs</span><span class="o">=</span><span class="n">initial_probs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">175</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;transitions&quot;</span><span class="p">],</span> <span class="n">props</span><span class="p">[</span><span class="s2">&quot;transitions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition_component</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">176</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;emissions&quot;</span><span class="p">],</span> <span class="n">props</span><span class="p">[</span><span class="s2">&quot;emissions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emission_component</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">emission_probs</span><span class="o">=</span><span class="n">emission_probs</span><span class="p">)</span>

<span class="nn">File ~/work/dynamax/dynamax/dynamax/hidden_markov_model/models/initial.py:45,</span> in <span class="ni">StandardHMMInitialState.initialize</span><span class="nt">(self, key, method, initial_probs)</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="k">if</span> <span class="n">initial_probs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span>     <span class="n">this_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">45</span>     <span class="n">initial_probs</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initial_probs_concentration</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">this_key</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span> <span class="c1"># Package the results into dictionaries</span>
<span class="g g-Whitespace">     </span><span class="mi">48</span> <span class="n">params</span> <span class="o">=</span> <span class="n">ParamsStandardHMMInitialState</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">initial_probs</span><span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1205,</span> in <span class="ni">Distribution.sample</span><span class="nt">(self, sample_shape, seed, name, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1190</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;Generate samples of the specified shape.</span>
<span class="g g-Whitespace">   </span><span class="mi">1191</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">1192</span><span class="sd"> Note that a call to `sample()` without arguments will generate a single</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">   </span><span class="mi">1202</span><span class="sd">   samples: a `Tensor` with prepended dimensions `sample_shape`.</span>
<span class="g g-Whitespace">   </span><span class="mi">1203</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">1204</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_and_control_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1205</span>   <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_sample_n</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1182,</span> in <span class="ni">Distribution._call_sample_n</span><span class="nt">(self, sample_shape, seed, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1178</span> <span class="n">sample_shape</span> <span class="o">=</span> <span class="n">ps</span><span class="o">.</span><span class="n">convert_to_shape_tensor</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1179</span>     <span class="n">ps</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sample_shape&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1180</span> <span class="n">sample_shape</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_sample_shape_to_vector</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1181</span>     <span class="n">sample_shape</span><span class="p">,</span> <span class="s1">&#39;sample_shape&#39;</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1182</span> <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_n</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1183</span>     <span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">()</span> <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">else</span> <span class="n">seed</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1184</span> <span class="n">samples</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1185</span>     <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ps</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">ps</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)),</span>
<span class="g g-Whitespace">   </span><span class="mi">1186</span>     <span class="n">samples</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1187</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_sample_static_shape</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">sample_shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/distributions/dirichlet.py:233,</span> in <span class="ni">Dirichlet._sample_n</span><span class="nt">(self, n, seed)</span>
<span class="g g-Whitespace">    </span><span class="mi">229</span> <span class="k">def</span> <span class="nf">_sample_n</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">230</span>   <span class="c1"># We use the log-space gamma sampler to avoid the bump-up-from-0 correction,</span>
<span class="g g-Whitespace">    </span><span class="mi">231</span>   <span class="c1"># and to apply the concentration &lt; 1 recurrence in log-space. This improves</span>
<span class="g g-Whitespace">    </span><span class="mi">232</span>   <span class="c1"># accuracy for small concentrations.</span>
<span class="ne">--&gt; </span><span class="mi">233</span>   <span class="n">log_gamma_sample</span> <span class="o">=</span> <span class="n">gamma_lib</span><span class="o">.</span><span class="n">random_gamma</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">234</span>       <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">concentration</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">concentration</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">log_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">235</span>   <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">236</span>       <span class="n">log_gamma_sample</span> <span class="o">-</span>
<span class="g g-Whitespace">    </span><span class="mi">237</span>       <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">log_gamma_sample</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/distributions/gamma.py:725,</span> in <span class="ni">random_gamma</span><span class="nt">(shape, concentration, rate, log_rate, seed, log_space)</span>
<span class="g g-Whitespace">    </span><span class="mi">723</span> <span class="k">def</span> <span class="nf">random_gamma</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">724</span>     <span class="n">shape</span><span class="p">,</span> <span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_space</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">725</span>   <span class="k">return</span> <span class="n">random_gamma_with_runtime</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">726</span>       <span class="n">shape</span><span class="p">,</span> <span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">log_rate</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">log_space</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/distributions/gamma.py:718,</span> in <span class="ni">random_gamma_with_runtime</span><span class="nt">(shape, concentration, rate, log_rate, seed, log_space)</span>
<span class="g g-Whitespace">    </span><span class="mi">713</span>   <span class="n">log_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">714</span> <span class="n">total_shape</span> <span class="o">=</span> <span class="n">ps</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">715</span>     <span class="p">[</span><span class="n">shape</span><span class="p">,</span> <span class="n">ps</span><span class="o">.</span><span class="n">broadcast_shape</span><span class="p">(</span><span class="n">ps</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concentration</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">716</span>                                <span class="n">_shape_or_scalar</span><span class="p">(</span><span class="n">rate</span><span class="p">,</span> <span class="n">log_rate</span><span class="p">))],</span>
<span class="g g-Whitespace">    </span><span class="mi">717</span>     <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">718</span> <span class="n">seed</span> <span class="o">=</span> <span class="n">samplers</span><span class="o">.</span><span class="n">sanitize_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">salt</span><span class="o">=</span><span class="s1">&#39;random_gamma&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">719</span> <span class="k">return</span> <span class="n">_random_gamma_gradient</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">720</span>     <span class="n">total_shape</span><span class="p">,</span> <span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">log_rate</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">log_space</span><span class="p">)</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/internal/samplers.py:144,</span> in <span class="ni">sanitize_seed</span><span class="nt">(seed, salt, name)</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span> <span class="k">if</span> <span class="n">salt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span>   <span class="n">salt</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hashlib</span><span class="o">.</span><span class="n">sha512</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">salt</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">144</span>   <span class="n">seed</span> <span class="o">=</span> <span class="n">fold_in</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">salt</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">146</span> <span class="k">if</span> <span class="n">JAX_MODE</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">147</span>   <span class="kn">import</span> <span class="nn">jax</span>  <span class="c1"># pylint: disable=g-import-not-at-top</span>

<span class="nn">File /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tensorflow_probability/substrates/jax/internal/samplers.py:186,</span> in <span class="ni">fold_in</span><span class="nt">(seed, salt)</span>
<span class="g g-Whitespace">    </span><span class="mi">183</span>   <span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">jaxrand</span>  <span class="c1"># pylint: disable=g-import-not-at-top</span>
<span class="g g-Whitespace">    </span><span class="mi">184</span>   <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>  <span class="c1"># pylint: disable=g-import-not-at-top</span>
<span class="g g-Whitespace">    </span><span class="mi">185</span>   <span class="k">return</span> <span class="n">jaxrand</span><span class="o">.</span><span class="n">fold_in</span><span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">186</span>       <span class="n">seed</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">salt</span> <span class="o">&amp;</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">SEED_DTYPE</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">187</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">salt</span><span class="p">,</span> <span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">integer_types</span><span class="p">)):</span>
<span class="g g-Whitespace">    </span><span class="mi">188</span>   <span class="n">seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">bitwise</span><span class="o">.</span><span class="n">bitwise_xor</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">189</span>       <span class="n">seed</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint64</span><span class="p">([</span><span class="n">salt</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">64</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

<span class="ne">OverflowError</span>: Python int too large to convert to C long
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice that <code class="docutils literal notranslate"><span class="pre">initialize</span></code> returns two things, the parameters and their properties. Among other things, the properties allow you to specify which parameters should be learned. You can set the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> flag to False if you want to fix certain parmeters.</p>
</div>
<section id="gradient-descent-is-a-special-case-of-stochastic-gradient-descent">
<h3>Gradient descent is a special case of <em>stochastic</em> gradient descent<a class="headerlink" href="#gradient-descent-is-a-special-case-of-stochastic-gradient-descent" title="Link to this heading">#</a></h3>
<p>Gradient descent is a special case of stochastic gradient descent (SGD) in which each iteration uses all the data to compute the descent direction for parameter updates. In contrast, SGD uses only a <em>minibatch</em> of data in each update. You can think of gradient descent as the special case where the minibatch is really the entire dataset. That’s why we sometimes call it <em>full batch</em> gradient descent. When you’re working with very large datasets (e.g. datasets with many sequences), however, minibatches can be very informative, and SGD can converge more quickly than full batch gradient descent.</p>
<p>Dynamax models have a <code class="docutils literal notranslate"><span class="pre">fit_sgd</span></code> function that runs SGD. If you want to run full batch gradient descent, all you have to do set <code class="docutils literal notranslate"><span class="pre">batch_size=num_batches</span></code>, as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fbgd_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">fbgd_params</span><span class="p">,</span> <span class="n">fbgd_losses</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">fit_sgd</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">,</span> 
                                       <span class="n">fbgd_props</span><span class="p">,</span> 
                                       <span class="n">batch_emissions</span><span class="p">,</span> 
                                       <span class="n">optimizer</span><span class="o">=</span><span class="n">optax</span><span class="o">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.95</span><span class="p">),</span>
                                       <span class="n">batch_size</span><span class="o">=</span><span class="n">num_batches</span><span class="p">,</span> 
                                       <span class="n">num_epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> 
                                       <span class="n">key</span><span class="o">=</span><span class="n">fbgd_key</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent-with-mini-batches">
<h3>Stochastic Gradient Descent with Mini-Batches<a class="headerlink" href="#stochastic-gradient-descent-with-mini-batches" title="Link to this heading">#</a></h3>
<p>Now let’s run it with stochastic gradient descent using a batch size of two sequences per mini-batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sgd_params</span><span class="p">,</span> <span class="n">sgd_param_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">sgd_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">sgd_params</span><span class="p">,</span> <span class="n">sgd_losses</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">fit_sgd</span><span class="p">(</span><span class="n">sgd_params</span><span class="p">,</span> 
                                     <span class="n">sgd_param_props</span><span class="p">,</span> 
                                     <span class="n">batch_emissions</span><span class="p">,</span> 
                                     <span class="n">optimizer</span><span class="o">=</span><span class="n">optax</span><span class="o">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.95</span><span class="p">),</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                     <span class="n">num_epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> 
                                     <span class="n">key</span><span class="o">=</span><span class="n">sgd_key</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fbgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;full batch GD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD (mini-batch size = 2)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Full Batch Gradient Descent Learning Curve&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9ac2aa2e56d05e019e13037eea3503edda5a40f9778b0ccd13b0a2dd2c2f6a62.png" src="../../_images/9ac2aa2e56d05e019e13037eea3503edda5a40f9778b0ccd13b0a2dd2c2f6a62.png" />
</div>
</div>
<p>As you can see, stochastic gradient descent converges much more quickly that full-batch gradient descent in this example. Intuitively, that’s because SGD takes multiple steps per <em>epoch</em> (i.e. each complete sweep through the dataset), whereas full-batch gradient descent takes only one.</p>
<p>The algorithms appear to have converged, but have they learned the correct parameters? Let’s see…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the parameters after learning</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Full batch gradient descent params:&quot;</span><span class="p">)</span>
<span class="n">print_params</span><span class="p">(</span><span class="n">fbgd_params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stochastic gradient descent params:&quot;</span><span class="p">)</span>
<span class="n">print_params</span><span class="p">(</span><span class="n">sgd_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Full batch gradient descent params:
initial probs:
[0.793 0.207]
transition matrix:
[[0.961 0.039]
 [0.292 0.708]]
emission probs:
[[0.162 0.142 0.115 0.131 0.145 0.306]
 [0.013 0.170 0.406 0.249 0.093 0.070]]

Stochastic gradient descent params:
initial probs:
[0.793 0.207]
transition matrix:
[[0.964 0.036]
 [0.344 0.656]]
emission probs:
[[0.165 0.143 0.124 0.130 0.150 0.287]
 [0.014 0.229 0.315 0.258 0.110 0.074]]
</pre></div>
</div>
</div>
</div>
<p>Ok, but not perfect!</p>
</section>
<section id="expectation-maximization">
<h3>Expectation-Maximization<a class="headerlink" href="#expectation-maximization" title="Link to this heading">#</a></h3>
<p>The more traditional way to estimate the parameters of an HMM is by expectation-maximization (EM). EM alternates between two steps:</p>
<ul class="simple">
<li><p><strong>E-step:</strong> Inferring the posterior distribution of latent states <span class="math notranslate nohighlight">\(z_{1:T}\)</span> given the parameters <span class="math notranslate nohighlight">\(\theta = (\pi, A, B)\)</span>. This step essentially runs the HMM forward-backward algorithm from the preceding notebook!</p></li>
<li><p><strong>M-step:</strong> Updating the parameters to maximize the expected log probability.
By iteratively performing these two steps, the algorithm converges to a local maximum of the marginal probability, <span class="math notranslate nohighlight">\(p(y_{1:T}, \theta)\)</span>, and hence to a MAP estimate of the parameters.</p></li>
</ul>
<p>In practice, EM often converges much quicker than SGD, especially when the models are “nice” (e.g. constructed with exponential family emission distributions). That is why dynamax has closed form M-steps for a variety of HMMs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">em_params</span><span class="p">,</span> <span class="n">em_param_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">em_params</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">fit_em</span><span class="p">(</span><span class="n">em_params</span><span class="p">,</span> 
                                  <span class="n">em_param_props</span><span class="p">,</span> 
                                  <span class="n">batch_emissions</span><span class="p">,</span> 
                                  <span class="n">num_iters</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='400' class='' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [400/400 00:01&lt;00:00]
    </div>
    </div></div>
</div>
</section>
<section id="compare-the-learning-curves">
<h3>Compare the learning curves<a class="headerlink" href="#compare-the-learning-curves" title="Link to this heading">#</a></h3>
<p>Finally, let’s compare the learning curve of EM to those of SGD. For comparison, we plot the loss associated with the true parameters that generated the data.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>To compare the log probabilities returned by <code class="docutils literal notranslate"><span class="pre">fit_em</span></code> to the losses returned by <code class="docutils literal notranslate"><span class="pre">fit_sgd</span></code>, you need to negate the log probabilities and divide by the total number of emissions. This is because optimization library defaults typically assume the loss is scaled to be <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the &quot;losses&quot; from EM </span>
<span class="n">em_losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_probs</span> <span class="o">/</span> <span class="n">batch_emissions</span><span class="o">.</span><span class="n">size</span> 

<span class="c1"># Compute the loss if you used the parameters that generated the data</span>
<span class="n">true_loss</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">hmm</span><span class="o">.</span><span class="n">marginal_log_prob</span><span class="p">,</span> <span class="n">params</span><span class="p">))(</span><span class="n">batch_emissions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">true_loss</span> <span class="o">+=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">true_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">true_loss</span> <span class="o">/</span> <span class="n">batch_emissions</span><span class="o">.</span><span class="n">size</span>

<span class="c1"># Plot the learning curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fbgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;full batch GD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD (mini-batch size = 2)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">em_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;EM&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_loss</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Params&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning Curve Comparison&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5c55850707208423386917493556f9a1d3d8b45d8314cc9f36ad9b8a0f70029e.png" src="../../_images/5c55850707208423386917493556f9a1d3d8b45d8314cc9f36ad9b8a0f70029e.png" />
</div>
</div>
<p>Not only does EM converge much faster on this example (here, in only a handful of iterations), it also converges to a better estimate of the parameters. Indeed, it essentially matches the loss obtained by the parameters that truly generated the data. We see that its parameter estimates are nearly the same as the true parameters, up to label switching.</p>
<p>(Label switching refers to the fact that the generated parameters assume state 1 corresponds to the loaded die, whereas the learned parameters assume this is state 0; since these solutions have the same likelihood, and since the prior is also symmetrical, there are two equally good posterior modes, and EM will just find one of them. When you compare inferred  parameters or states between models, you may need to use our <a class="reference external" href="https://probml.github.io/dynamax/api.html#dynamax.utils.utils.find_permutation">find_permutation</a> function to find the best correspondence between discrete latent labels.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_params</span><span class="p">(</span><span class="n">em_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="nn">/Users/kpmurphy/github/dynamax/docs/notebooks/hmm/casino_hmm_learning.ipynb Cell 27</span> in <span class="ni">&lt;cell line: 1&gt;</span><span class="nt">()</span>
<span class="o">----&gt;</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/Users/kpmurphy/github/dynamax/docs/notebooks/hmm/casino_hmm_learning.ipynb#X35sZmlsZQ%3D%3D?line=0&#39;</span><span class="o">&gt;</span><span class="mi">1</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">print_params</span><span class="p">(</span><span class="n">em_params</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;print_params&#39; is not defined
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>This notebook showed how to learn the parameters of an HMM using SGD (with full batch or with mini-batches) and EM. For many HMMs, especially the exponential family HMMs with exact M-steps implemented in Dynamax, EM tends to converge very quickly.</p>
<p>This notebook glossed over some important details:</p>
<ul class="simple">
<li><p>Both SGD and EM are prone to getting stuck in local optima. For example, if you change the key for the random initialization, you may find that the learned parameters are not as good. There are a few ways around that problem. One is to use a heuristic to initialize the parameters more intelligently. Another is to use many random initializations of the model and keep the one that achieves the best loss.</p></li>
<li><p>This notebook did not address the important question of <em>how to determine the number of discrete states</em>. We often use cross-validation for that purpose, as described next.</p></li>
</ul>
<p>So far, we have focused on HMMs with discrete emissions from a categorical distribution. The next notebook will illustrate a Gaussian HMM for continuous data. We will also discuss some of the concerns above.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="casino_hmm_inference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Casino HMM: Inference (state estimation)</p>
      </div>
    </a>
    <a class="right-next"
       href="gaussian_hmm.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gaussian HMM: Cross-validation and Model Selection</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-data-from-true-model">Sample data from true model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-with-gradient-descent">Learning with Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-is-a-special-case-of-stochastic-gradient-descent">Gradient descent is a special case of <em>stochastic</em> gradient descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-with-mini-batches">Stochastic Gradient Descent with Mini-Batches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization">Expectation-Maximization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-the-learning-curves">Compare the learning curves</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, and Kevin Murphy
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022, Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, and Kevin Murphy.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>