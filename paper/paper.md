---
title: 'Dynamax: A Python package for probabilistic state space models (SSMs) written
in JAX'
tags:
  - Python
  - State space models
  - dynamics
  - JAX

Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, Kevin Murphy.

authors:
  - name: Scott W. Linderman
    orcid: 0000-0002-3878-9073
    affiliation: "1, 2" # (Multiple affiliations must be quoted)
    corresponding: true # (This is how to denote the corresponding author)
  - name: Peter Chang
    affiliation: "3"
  - name: Giles Harper-Donnelly
    affiliation: "4"
  - name: Aleyna Kara
    affiliation: "5"
  - name: Xinglong Li
    affiliation: "6"
  - name: Kevin Murphy
    affiliation: "2"
affiliations:
 - name: Department of Statistics and Wu Tsai Neurosciences Insitute, Stanford University, USA
   index: 1
 - name: Google Research, USA
   index: 2
 - name: CSAIL, Massachusetts Institute of Technology, USA
   index: 3
 - name: Cambridge University, UK
   index: 4
 - name: Boğaziçi University, Turkey
   index: 5
 - name: University of British Columbia, Canada
   index: 6
 
date: 12 July 2024
bibliography: paper.bib

---

# Summary

Probabilistic state space models (SSMs) are fundamental tools for modeling
sequential data, and they are broadly used in many engineering and scientific
disciplines. Let $y_1, \ldots y_T$ denote a sequence of observations where 
$y_t$ denotes the observation at time $t$. In an SSM, the observations are 
generated by a latent state, $z_t$, which evolve according to a transition 
(aka dynamics) model. An SSM may also use inputs (aka controls or covariates), 
$u_t$, to steer the latent state dynamics and influence the observations.

For example, SSMs are often used in neuroscience to model the dynamics of 
neural spike train recordings [@vyas2020computation]. Here, $y_t$ is a vector of spike 
counts from each of, say, 100 measured neurons. The activity of nearby neurons 
is often correlated, and SSMs can capture that correlation through a lower 
dimensional latent state, $z_t$. Finally, if we know that certain sensory inputs 
may drive the neural activity, we can encode them in $u_t$. A common goal
in neuroscience is to infer the latent states $z_t$ that best explain the 
observed neural spike train; this is called _state inference_. Another goal 
is to estimate the dynamics that govern how latent states evolve over time; this is 
called _parameter estimation_. `Dynamax` provides algorithms for state inference
and parameter estimation in a variety of SSMs. 

The key design choices when constructing an SSM include the type of latent state 
(is $z_t$ a continuous or discrete random variable?), the dynamics that govern 
how latent states evolve over time (are they linear or nonlinear?), and the 
link between latent states, inputs, and observations. Canonical examples of SSMs
include hidden Markov models (HMM), which have discrete latent states, and
linear dynamical systems (LDS), which have continuous latent states with 
linear dynamics and additive Gaussian noise. `Dynamax` supports these canonical
examples as well as more complex models.

More information about state space models and algorithms for state inference
and parameter estimation can be found in @murphy2023probabilistic and @sarkka2023bayesian. 


# Statement of need

`Dynamax` is an open-source Python pacakge for state space modeling. Since it 
is built with `JAX` [@jax], it automatically supports just-in-time (JIT)
compilation for hardware acceleration on CPU, GPU, and TPU machines. 
It also supports automatic differentiation for gradient-based model learning.
While other libraries exist for state space modeling in Python, and some also
use `JAX`, this library provides a combination of low-level inference
algorithms and high-level modeling objects that can support a wide range of
research applications.

The API for `Dynamax` is divided into two parts: a set of core, functionally
pure, low-level inference algorithms, and a high-level, object oriented module
for constructing and fitting probabilistic SSMs. 
The low-level inference API provides message passing algorithms for several
common types of SSMs. For example, `Dynamax` provides `JAX` implementations for:
- Forward-Backward algorithms for discrete-state hidden Markov models (HMMs), 
- Kalman filtering and smoothing algorithms for linear Gaussian SSMs, 
- Extended and unscented Kalman filtering and smoothing for nonlinear Gaussian SSMs, and
- Conditional moment filtering and smoothing algorithms for models with non-Gaussian emissions. 
- Parallel message passing routines take advantage of GPU or TPU acceleration to perform message passing in sublinear time. 
The high-level model API makes it easy to construct, fit, and inspect HMMs and
linear Gaussian SSMs.

`Dynamax` has supported several publications. The low-level API has been used 
in machine learning research [@zhao2023revisiting; @lee2023switching; @chang2023low]. 
More sophisticated, special purpose models on top of `Dynamax`, like the 
Keypoint-MoSeq library for modeling postural dynamics
of animals [@weinreb2024keypoint]. Finally, the `Dynamax` tutorials are used as reference 
examples in a major machine learning textbook [@murphy2023probabilistic]. 

# Acknowledgements

Most of this library was developed while S.W.L. was a Visiting Faculty Researcher
at Google and P.C., G.H.D., A.K., and X.L. were Google Summer of Code participants. 

# References